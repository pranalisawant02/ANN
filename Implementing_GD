#exp2....gradient decent using step activation function
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])

w = 0
b = 0

learning_rate = 0.01
num_iterations = 1000

for i in range(num_iterations):
    y_pred = w * x + b
    error = y_pred - y

    dw = (2/len(x)) * np.sum(error * x)
    db = (2/len(x)) * np.sum(error)

    w = w - learning_rate * dw
    b = b - learning_rate * db

    if i % 100 == 0:
        print(f"Iteration {i}: w = {w:.4f}, b = {b:.4f}, MSE = {np.mean(error**2):.4f}")

plt.scatter(x, y, color='blue', label='True values')
plt.plot(x, w * x + b, color='red', label='Fitted line')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Regression with Gradient Descent')
plt.legend()
plt.show()

print(f"Final weight (w) = {w:.4f}, Final bias (b) = {b:.4f}")
